{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import TripletNetwork, FasterRCNNEmbedder\n",
    "from src.data import *\n",
    "from src.transforms import albumentations_transform\n",
    "\n",
    "from torch.nn import TripletMarginLoss\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, CSVLogger\n",
    "\n",
    "# Initialize feature extractor, model, loss, optimizer, lr_scheduler\n",
    "\n",
    "model = FasterRCNNEmbedder()\n",
    "loss = TripletMarginLoss(margin=1.0, p=2)\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "lr_sceduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=0.0001, last_epoch=-1)\n",
    "\n",
    "# initialize TripletNetwork for training\n",
    "network = TripletNetwork(model,\n",
    "                          loss,\n",
    "                          optimizer,\n",
    "                          lr_sceduler)\n",
    "\n",
    "# initialize datamodule\n",
    "\n",
    "dm = TripletDataModule(data_dir='/home/georg/projects/university/C5/task3/dataset/COCO',\n",
    "                          json_file='/home/georg/projects/university/C5/task3/dataset/COCO/mcv_image_retrieval_annotations.json',\n",
    "                          batch_size=96,\n",
    "                          #transforms=albumentations_transform(),\n",
    "                          num_workers=16,\n",
    "                          dims=(224, 224))\n",
    "\n",
    "# Initialize callbacks \n",
    "checkpointer = ModelCheckpoint(\n",
    "    monitor=\"val_loss\", save_top_k=1, mode=\"min\", save_weights_only=True)\n",
    "early_stopper = EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n",
    "logger = CSVLogger(\"logs\", name=\"TripletNetworkCSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the network\n",
    "\n",
    "# trainer = pl.Trainer(max_epochs=20, \n",
    "#                     devices=1,\n",
    "#                     accelerator='gpu',\n",
    "#                     callbacks=[checkpointer, early_stopper],\n",
    "#                     logger=logger,\n",
    "#                     num_sanity_val_steps=0) \n",
    "# trainer.fit(network, dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper functions to order the data \n",
    "\n",
    "def get_img_file_name(img_id, set):\n",
    "    return 'COCO_{}2014_{:012d}.jpg'.format(set, img_id)\n",
    "\n",
    "def prepare_data(json_file, mode):\n",
    "        with open(json_file, 'r') as file:\n",
    "            # Load the JSON data\n",
    "            data = json.load(file)[mode]\n",
    "        print(f'Loaded {len(data)} classes from {json_file}')\n",
    "        img_ids = []\n",
    "        labels = []\n",
    "        # loop over classes \n",
    "        for key in tqdm(data.keys(), desc=f'Preparing {mode} data'):\n",
    "            class_ = key\n",
    "            images_with_class = data[key]\n",
    "            # loop over images with the class\n",
    "            for image_id in images_with_class:\n",
    "                # if it's a new image, add it to the list of images and create a label list for it\n",
    "                if image_id not in img_ids:\n",
    "                    img_ids.append(image_id)\n",
    "                    labels.append([])\n",
    "            # loop over images and add the class to the label list if it's in the list of images\n",
    "            for i, img_id in enumerate(img_ids):\n",
    "                if img_id in images_with_class:\n",
    "                    labels[i].append(int(class_))\n",
    "\n",
    "        data_split = 'train' if mode in ['train', 'database'] else 'val'\n",
    "\n",
    "        img_files = [get_img_file_name(img_id, data_split) for img_id in img_ids]\n",
    "        return img_files, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "#load model from checkpoint and set to eval mode\n",
    "model.load_state_dict(torch.load('/home/georg/projects/university/C5/task3/task_3e/logs/TripletNetworkCSV/version_19/checkpoints/epoch=8-step=7695.ckpt'), strict=False)\n",
    "model.eval()\n",
    "\n",
    "# specify json file path\n",
    "data_json = '/home/georg/projects/university/C5/task3/dataset/COCO/mcv_image_retrieval_annotations.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 80 classes from /home/georg/projects/university/C5/task3/dataset/COCO/mcv_image_retrieval_annotations.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing database data: 100%|██████████| 80/80 [00:00<00:00, 5942.94it/s]\n",
      "  0%|          | 0/1959 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1959/1959 [01:30<00:00, 21.65it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "from PIL import Image\n",
    "from src.transforms import preprocess\n",
    "import numpy as np\n",
    "\n",
    "# define helper functions to extract embeddings from images using the model\n",
    "def extract_embeddings(img_files, imgs_path, model):\n",
    "    embeddings = []\n",
    "    for img_file in tqdm(img_files):\n",
    "        img_path = os.path.join(imgs_path, img_file)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = preprocess([224,224])(image)\n",
    "        image = image.unsqueeze(0)\n",
    "        pred = model(image)\n",
    "        embeddings.append(pred.squeeze(0).cpu().detach().numpy())\n",
    "    return np.array(embeddings)\n",
    "\n",
    "\n",
    "# extract embeddings from the training images\n",
    "train_imgs_path = '/home/georg/projects/university/C5/task3/dataset/COCO/train2014'\n",
    "train_img_files, train_labels = prepare_data(json_file=data_json, mode='database')\n",
    "train_embeddings = extract_embeddings(train_img_files, train_imgs_path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1959, 1024)\n",
      "1959\n"
     ]
    }
   ],
   "source": [
    "# Create FAISS index and add the training embeddings to it\n",
    "import faiss  \n",
    "             \n",
    "index = faiss.IndexFlatL2(1024)   # build the index, d=size of vectors \n",
    "faiss.normalize_L2(train_embeddings)\n",
    "print(train_embeddings.shape)\n",
    "index.add(train_embeddings)                 # add vectors to the index\n",
    "print(index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 80 classes from /home/georg/projects/university/C5/task3/dataset/COCO/mcv_image_retrieval_annotations.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing test data: 100%|██████████| 80/80 [00:00<00:00, 5519.27it/s]\n",
      "  0%|          | 0/1917 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1917/1917 [01:25<00:00, 22.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  25   26   90   43   47]\n",
      " [1125   38 1117 1118 1129]\n",
      " [ 386  836  777  371   68]\n",
      " ...\n",
      " [ 400 1535 1374  548 1484]\n",
      " [1656 1695 1892  403 1281]\n",
      " [1117  854  466 1506  764]]\n"
     ]
    }
   ],
   "source": [
    "# Extract embeddings from the test/val images (can be configured using 'mode')\n",
    "\n",
    "val_imgs_path = '/home/georg/projects/university/C5/task3/dataset/COCO/val2014'\n",
    "val_img_files, val_labels = prepare_data(json_file=data_json, mode='test')\n",
    "val_embeddings = extract_embeddings(val_img_files, val_imgs_path, model)\n",
    "\n",
    "# Search for similar vectors k in the FAISS index\n",
    "k = 5                       # we want 4 similar vectors\n",
    "D, I = index.search(val_embeddings, k)     # actual search\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1917\n",
      "1917\n",
      "0.5101721439749609\n",
      "Precision: 1.0, \n",
      "Recall: 0.5101721439749609, \n",
      "Accuracy: 0.5101721439749609, \n",
      "F1: 0.6756476683937824\n",
      "Average precision: 1.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score, average_precision_score\n",
    "\n",
    "\n",
    "k = 5\n",
    "visualize = False\n",
    "\n",
    "targets = []\n",
    "preds = []\n",
    "for i, retrieval_indices in enumerate(I):\n",
    "    query_img_file = val_img_files[i]\n",
    "    query_img_path = os.path.join(val_imgs_path, query_img_file)\n",
    "    query_img_labels = val_labels[i]\n",
    "    retrieved_image_files = []\n",
    "    retrieved_image_labels = []\n",
    "    targets.append(1)\n",
    "\n",
    "    for train_idx in retrieval_indices[:k]:\n",
    "        retrieved_image_files.append(train_img_files[train_idx])\n",
    "        retrieved_image_paths = [os.path.join(train_imgs_path, file) for file in retrieved_image_files]\n",
    "        retrieved_image_labels.extend(train_labels[train_idx])\n",
    "    \n",
    "    \n",
    "    preds.append(len(set(query_img_labels).intersection(set(retrieved_image_labels)))>0)\n",
    "    if visualize ==True:\n",
    "        query_img = cv2.imread(query_img_path)\n",
    "        query_img = cv2.resize(query_img, (224, 224))\n",
    "        retrieved_imgs = [cv2.imread(file) for file in retrieved_image_paths]\n",
    "        retrieved_imgs = [cv2.resize(img, (224, 224)) for img in retrieved_imgs]\n",
    "        \n",
    "        fig, ax = plt.subplots(1, 6, figsize=(18, 3))\n",
    "        ax[0].imshow(cv2.cvtColor(query_img, cv2.COLOR_BGR2RGB))\n",
    "        ax[0].set_title('Query Image')\n",
    "        for j, img in enumerate(retrieved_imgs):\n",
    "            ax[j+1].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            ax[j+1].set_title(f'Retrieved Image {j+1}')\n",
    "        plt.savefig('query_plots/{:03d}.png'.format(i))\n",
    "        plt.close()\n",
    "\n",
    "print(len(targets))\n",
    "print(len(preds))\n",
    "precision = precision_score(targets, preds, average='binary')\n",
    "recall = recall_score(targets, preds, average='binary')\n",
    "accuracy = accuracy_score(targets, preds)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(f'Precision: {precision}, \\nRecall: {recall}, \\nAccuracy: {accuracy}, \\nF1: {f1}')\n",
    "average_precision = average_precision_score(targets, preds)\n",
    "print(f'Average precision: {average_precision}')\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
